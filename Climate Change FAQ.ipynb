{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d7bc42c-ef32-44e4-9901-1d3a39e60ccf",
   "metadata": {},
   "source": [
    "# Language Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ad1605-c1cc-437b-94c0-9860b3b2aa46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practice\n",
    "# https://www.kaggle.com/datasets/kreeshrajani/3k-conversations-dataset-for-chatbot?select=Conversation.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "035f6034-665b-48dc-8f29-68c05bf6d941",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\milto\\anaconda3\\envs\\torch_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import zipfile\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "511826b5-43c2-47f4-a3e8-da4001ff4f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 376 entries, 0 to 375\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   source     376 non-null    object\n",
      " 1   faq        376 non-null    object\n",
      " 2   text_type  376 non-null    object\n",
      "dtypes: object(3)\n",
      "memory usage: 8.9+ KB\n",
      "None\n",
      "\n",
      "First 5 Rows:\n",
      "                                              source  \\\n",
      "0  https://www.ipcc.ch/site/assets/uploads/2020/0...   \n",
      "1  https://www.ipcc.ch/site/assets/uploads/2020/0...   \n",
      "2  https://www.ipcc.ch/site/assets/uploads/2020/0...   \n",
      "3  https://www.ipcc.ch/site/assets/uploads/2020/0...   \n",
      "4  https://www.ipcc.ch/site/assets/uploads/2020/0...   \n",
      "\n",
      "                                                 faq text_type  \n",
      "0  If Understanding of the Climate System Has Inc...         q  \n",
      "1  The models used to calculate the IPCC’s temper...         a  \n",
      "2               How Do We Know the World Has Warmed?         q  \n",
      "3  Evidence for a warming world comes from multip...         a  \n",
      "4   Have There Been Any Changes in Climate Extremes?         q  \n"
     ]
    }
   ],
   "source": [
    "# Step 1: Extract and Load the Dataset\n",
    "def load_and_inspect_dataset(zip_file_path, csv_file_name):\n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(\"extracted_files\")\n",
    "    \n",
    "    csv_file_path = f\"extracted_files/{csv_file_name}\"\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "    \n",
    "    # Display basic information\n",
    "    print(\"Dataset Info:\")\n",
    "    print(df.info())\n",
    "    print(\"\\nFirst 5 Rows:\")\n",
    "    print(df.head())\n",
    "    \n",
    "    return df\n",
    "\n",
    "zip_file_path = \"Climate Change FAQs.zip\"\n",
    "csv_file_name = \"climate_change_faqs.csv\"\n",
    "df = load_and_inspect_dataset(zip_file_path, csv_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67cf36e7-2a00-4108-a52b-63f43ebd24b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample Preprocessed Questions:\n",
      "['if understanding of the climate system has increased, why hasn t the range of temperature projections been reduced?', 'how do we know the world has warmed?', 'have there been any changes in climate extremes?']\n",
      "\n",
      "Sample Preprocessed Answers:\n",
      "['the models used to calculate the ipcc s temperature projections agree on the direction of future global change, but the projected size of those changes cannot be precisely predicted. future greenhouse gas ghg emission rates could take any one of many possible trajectories, and some underlying physical processes are not yet completely understood, making them difficult to model. those uncertainties, combined with natural year to year climate variability, produce an uncertainty range in temperature projections. the uncertainty range around projected ghg and aerosol precursor emissions which depend on projections of future social and economic conditions cannot be materially reduced. nevertheless, improved understanding and climate models along with observational constraints may reduce the uncertainty range around some factors that influence the climate s response to those emission changes. the complexity of the climate system, however, makes this a slow process. climate science has made many important advances since the last ipcc assessment report, thanks to improvements in measurements and data analysis in the cryosphere, atmosphere, land, biosphere and ocean systems. scientists also have better understanding and tools to model the role of clouds, sea ice, aerosols, small scale ocean mixing, the carbon cycle and other processes. more observations mean that models can now be evaluated more thoroughly, and projections can be better constrained. for example, as models and observational analysis have improved, projections of sea level rise have become more accurate, balancing the current sea level rise budget. despite these advances, there is still a range in plausible projections for future global and regional climate what scientists call an uncertainty range . these uncertainty ranges are specific to the variable being considered precipitation vs. temperature, for instance and the spatial and temporal extent such as regional vs. global averages . uncertainties in climate projections arise from natural variability and uncertainty around the rate of future emissions and the climate s response to them. they can also occur because representations of some known processes are as yet unrefined, and because some processes are not included in the models. there are fundamental limits to just how precisely annual temperatures can be projected, because of the chaotic nature of the climate system. furthermore, decadal scale projections are sensitive to prevailing conditions such as the temperature of the deep ocean that are less well known. some natural variability over decades arises from interactions between the ocean, atmosphere, land, biosphere and cryosphere, and is also linked to phenomena such as the el ni o southern oscillation enso and the north atlantic oscillation see box 2.5 for details on patterns and indices of climate variability . volcanic eruptions and variations in the sun s output also contribute to natural variability, although they are externally forced and explainable. this natural variability can be viewed as part of the noise in the climate record, which provides the backdrop against which the signal of anthropogenic climate change is detected. natural variability has a greater influence on uncertainty at regional and local scales than it does over continental or global scales. it is inherent in the earth system, and more knowledge will not eliminate the uncertainties it brings. however, some progress is possible particularly for projections up to a few years ahead which exploit advances in knowledge of, for instance, the cryosphere or ocean state and processes. this is an area of active research. when climate variables are averaged over decadal timescales or longer, the relative importance of internal variability diminishes, making the long term signals more evident faq1.1, figure 1 . this long term perspective is consistent with a common definition of climate as an average over 30 years. a second source of uncertainty stems from the many possible trajectories that future emission rates of ghgs and aerosol precursors might take, and from future trends in land use. nevertheless, climate projections rely on input from these variables. so to obtain these estimates, scientists consider a number of alternative scenarios for future human society, in terms of population, economic and technological change, and political choices. they then estimate the likely emissions under each scenario. the ipcc informs policymaking, therefore climate projections for different emissions scenarios can be useful as they show the possible climatic consequences of different policy choices. these scenarios are intended to be compatible with the full range of emissions scenarios described in the current scientific literature, with or without climate policy. as such, they are designed to sample uncertainty in future scenarios. projections for the next few years and decades are sensitive to emissions of short lived compounds such as aerosols and methane. more distant projections, however, are more sensitive to alternative scenarios around long lived ghg emissions. these scenario dependent uncertainties will not be reduced by improvements in climate science, and will become the dominant uncertainty in projections over longer timescales e.g., 2100 . the final contribution to the uncertainty range comes from our imperfect knowledge of how the climate will respond to future anthropogenic emissions and land use change. scientists principally use computer based global climate models to estimate this response. a few dozen global climate models have been developed by different groups of scientists around the world. all models are built on the same physical principles, but some approximations are needed because the climate system is so complex. different groups choose slightly different approximations to represent specific processes in the atmosphere, such as clouds. these choices produce differences in climate projections from different models. this contribution to the uncertainty range is described as response uncertainty or model uncertainty . the complexity of the earth system means that future climate could follow many different scenarios, yet still be consistent with current understanding and models. as observational records lengthen and models improve, researchers should be able, within the limitations of the range of natural variability, to narrow that range in probable temperature in the next few decades. it is also possible to use information about the current state of the oceans and cryosphere to produce better projections up to a few years ahead. as science improves, new geophysical processes can be added to climate models, and representations of those already included can be improved. these developments can appear to increase model derived estimates of climate response uncertainty, but such increases merely reflect the quantification of previously unmeasured sources of uncertainty. as more and more important processes are added, the influence of unquantified processes lessens, and there can be more confidence in the projections.', 'evidence for a warming world comes from multiple independent climate indicators, from high up in the atmosphere to the depths of the oceans. they include changes in surface, atmospheric and oceanic temperatures glaciers snow cover sea ice sea level and atmospheric water vapour. scientists from all over the world have independently verified this evidence many times. that the world has warmed since the 19th century is unequivocal. discussion about climate warming often centres on potential residual biases in temperature records from landbased weather stations. these records are very important, but they only represent one indicator of changes in the climate system. broader evidence for a warming world comes from a wide range of independent physically consistent measurements of many other, strongly interlinked, elements of the climate system . a rise in global average surface temperatures is the best known indicator of climate change. although each year and even decade is not always warmer than the last, global surface temperatures have warmed substantially since 1900. warming land temperatures correspond closely with the observed warming trend over the oceans. warming oceanic air temperatures, measured from aboard ships, and temperatures of the sea surface itself also coincide, as borne out by many independent analyses. the atmosphere and ocean are both fluid bodies, so warming at the surface should also be seen in the lower atmosphere, and deeper down into the upper oceans, and observations confirm that this is indeed the case. analyses of measurements made by weather balloon radiosondes and satellites consistently show warming of the troposphere, the active weather layer of the atmosphere. more than 90 of the excess energy absorbed by the climate system since at least the 1970s has been stored in the oceans as can be seen from global records of ocean heat content going back to the 1950s. as the oceans warm, the water itself expands. this expansion is one of the main drivers of the independently observed rise in sea levels over the past century. melting of glaciers and ice sheets also contribute, as do changes in storage and usage of water on land. a warmer world is also a moister one, because warmer air can hold more water vapour. global analyses show that specific humidity, which measures the amount of water vapour in the atmosphere, has increased over both the land and the oceans. the frozen parts of the planet known collectively as the cryosphere affect, and are affected by, local changes in temperature. the amount of ice contained in glaciers globally has been declining every year for more than 20 years, and the lost mass contributes, in part, to the observed rise in sea level. snow cover is sensitive to changes in temperature, particularly during the spring, when snow starts to melt. spring snow cover has shrunk across the nh since the 1950s. substantial losses in arctic sea ice have been observed since satellite records began, particularly at the time of the mimimum extent, which occurs in september at the end of the annual melt season. by contrast, the increase in antarctic sea ice has been smaller. individually, any single analysis might be unconvincing, but analysis of these different indicators and independent data sets has led many independent research groups to all reach the same conclusion. from the deep oceans to the top of the troposphere, the evidence of warmer air and oceans, of melting ice and rising seas all points unequivocally to one thing the world has warmed since the late 19th century.', 'there is strong evidence that warming has lead to changes in temperature extremes including heat waves since the mid 20th century. increases in heavy precipitation have probably also occurred over this time, but vary by region. however, for other extremes, such as tropical cyclone frequency, we are less certain, except in some limited regions, that there have been discernable changes over the observed record. from heat waves to cold snaps or droughts to flooding rains, recording and analysing climate extremes poses unique challenges, not just because these events are rare, but also because they invariably happen in conjunction with disruptive conditions. furthermore, there is no consistent definition in the scientific literature of what constitutes an extreme climatic event, and this complicates comparative global assessments. although, in an absolute sense, an extreme climate event will vary from place to place a hot day in the tropics, for instance, may be a different temperature to a hot day in the mid latitudes international efforts to monitor extremes have highlighted some significant global changes. for example, using consistent definitions for cold 10th percentile and warm 90th percentile days and nights it is found that warm days and nights have increased and cold days and nights have decreased for most regions of the globe a few exceptions being central and eastern north america, and southern south america but mostly only related to daytime temperatures. those changes are generally most apparent in minimum temperature extremes, for example, warm nights. data limitations make it difficult to establish a causal link to increases in average temperatures, but, indicates that daily global temperature extremes have indeed changed. whether these changes are simply associated with the average of daily temperatures increasing or whether other changes in the distribution of daytime and nighttime temperatures have occurred is still under debate. warm spells or heat waves, that is, periods containing consecutive extremely hot days or nights, have also been assessed, but there are fewer studies of heat wave characteristics than those that compare changes in merely warm days or nights. most global land areas with available data have experienced more heat waves since the middle of the 20th century. one exception is the south eastern usa, where heat wave frequency and duration measures generally show decreases. this has been associated with a so called warming hole in this region, where precipitation has also increased and may be related to interactions between the land and the atmosphere and long term variations in the atlantic and pacific oceans. however, for large regions, particularly in africa and south america, information on changes in heatwaves is limited. for regions such as europe, where historical temperature reconstructions exist going back several hundreds of years, indications are that some areas have experienced a disproportionate number of extreme heat waves in recent decades. changes in extremes for other climate variables are generally less coherent than those observed for temperature, owing to data limitations and inconsistencies between studies, regions and or seasons. however, increases in precipitation extremes, for example, are consistent with a warmer climate. analyses of land areas with sufficient data indicate increases in the frequency and intensity of extreme precipitation events in recent decades, but results vary strongly between regions and seasons. for instance, evidence is most compelling for increases in heavy precipitation in north america, central america and europe, but in some other regions such as southern australia and western asia there is evidence of decreases. likewise, drought studies do not agree on the sign of the global trend, with regional inconsistencies in trends also dependent on how droughts are defined. however, indications exist that droughts have increased in some regions e.g., the mediterranean and decreased in others e.g., central north america since the middle of the 20th century. considering other extremes, such as tropical cyclones, the latest assessments show that due to problems with past observing capabilities, it is difficult to make conclusive statements about long term trends. there is very strong evidence, however, that storm activity has increased in the north atlantic since the 1970s. over periods of a century or more, evidence suggests slight decreases in the frequency of tropical cyclones making landfall in the north atlantic and the south pacific, once uncertainties in observing methods have been considered. little evidence exists of any longer term trend in other ocean basins. for extratropical cyclones, a poleward shift is evident in both hemispheres over the past 50 years, with further but limited evidence of a decrease in wind storm frequency at mid latitudes. several studies suggest an increase in intensity, but data sampling issues hamper these assessments. summarizes some of the observed changes in climate extremes. overall, the most robust global changes in climate extremes are seen in measures of daily temperature, including to some extent, heat waves. precipitation extremes also appear to be increasing, but there is large spatial variability, and observed trends in droughts are still uncertain except in a few regions. while robust increases have been seen in tropical cyclone frequency and activity in the north atlantic since the 1970s, the reasons for this are still being debated. there is limited evidence of changes in extremes associated with other climate variables since the mid 20th century.']\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Preprocess the Data\n",
    "def preprocess_data(df):\n",
    "    import re\n",
    "\n",
    "    # Clean text data\n",
    "    def clean_text(text):\n",
    "        text = text.lower()  # Convert to lowercase\n",
    "        text = re.sub(r\"[^a-zA-Z0-9?.!,¿]+\", \" \", text)  # Remove unwanted characters\n",
    "        return text.strip()\n",
    "\n",
    "    df['faq'] = df['faq'].apply(clean_text)\n",
    "\n",
    "    # Split the data into questions and answers\n",
    "    questions = df[df['text_type'] == 'q']['faq'].tolist()\n",
    "    answers = df[df['text_type'] == 'a']['faq'].tolist()\n",
    "\n",
    "    # Validate lengths\n",
    "    if len(questions) != len(answers):\n",
    "        raise ValueError(\"Mismatch between the number of questions and answers.\")\n",
    "    \n",
    "    # Log sample data\n",
    "    print(\"\\nSample Preprocessed Questions:\")\n",
    "    print(questions[:3])\n",
    "    print(\"\\nSample Preprocessed Answers:\")\n",
    "    print(answers[:3])\n",
    "\n",
    "    return questions, answers\n",
    "\n",
    "questions, answers = preprocess_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68c811b7-8f67-4ef7-8e6e-3a00b3326d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Samples: 150, Testing Samples: 38\n",
      "\n",
      "Sample Training Pair:\n",
      "('why does the temperature record shown on your vital signs page begin at 1880?', 'three of the world s most complete temperature tracking records from nasa s goddard institute for space studies, the national oceanic and atmospheric administration s national climactic data center and the uk meteorological office s hadley centre begin in 1880. prior to 1880, temperature measurements were made with instruments like thermometers. the oldest continuous temperature record is the central england temperature data series, which began in 1659, and the hadley centre has some measurements beginning in 1850, but there are too few data before 1880 for scientists to estimate average temperatures for the entire planet.')\n",
      "Model and tokenizer loaded successfully from microsoft/DialoGPT-small!\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Prepare Training and Testing Data\n",
    "def prepare_data(questions, answers, test_size=0.2, random_state=42):\n",
    "    pairs = list(zip(questions, answers))\n",
    "    train_pairs, test_pairs = train_test_split(pairs, test_size=test_size, random_state=random_state)\n",
    "    \n",
    "    # Log data statistics\n",
    "    print(f\"\\nTraining Samples: {len(train_pairs)}, Testing Samples: {len(test_pairs)}\")\n",
    "    print(\"\\nSample Training Pair:\")\n",
    "    print(train_pairs[0])\n",
    "    \n",
    "    return train_pairs, test_pairs\n",
    "\n",
    "train_pairs, test_pairs = prepare_data(questions, answers)# Step 4: Load Pre-Trained Chatbot Model\n",
    "def load_chatbot_model(model_name=\"microsoft/DialoGPT-small\"):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    \n",
    "    # Set pad_token if not set\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token  # Use eos_token as pad_token\n",
    "    \n",
    "    print(f\"Model and tokenizer loaded successfully from {model_name}!\")\n",
    "    return tokenizer, model\n",
    "\n",
    "tokenizer, model = load_chatbot_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37d99e24-aaaf-4a65-b797-58d2d18a8ccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer loaded successfully from microsoft/DialoGPT-small!\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Load Pre-Trained Chatbot Model\n",
    "def load_chatbot_model(model_name=\"microsoft/DialoGPT-small\"):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    \n",
    "    # Set pad_token if not set\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token  # Use eos_token as pad_token\n",
    "    \n",
    "    print(f\"Model and tokenizer loaded successfully from {model_name}!\")\n",
    "    return tokenizer, model\n",
    "\n",
    "tokenizer, model = load_chatbot_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e13db14a-0a8e-45f1-b4f6-1b51d437c1e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Debugging Train Loader:\n",
      "Batch Input IDs Shape: torch.Size([8, 16])\n",
      "Batch Target IDs Shape: torch.Size([8, 128])\n"
     ]
    }
   ],
   "source": [
    "# Ensure tokenizer has a pad_token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Step 1: Create the ConversationDataset\n",
    "class ConversationDataset(Dataset):\n",
    "    def __init__(self, pairs, tokenizer, max_length=128):\n",
    "        self.pairs = pairs\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        question, answer = self.pairs[idx]\n",
    "        input_ids = self.tokenizer.encode(\n",
    "            question, truncation=True, max_length=self.max_length, return_tensors=\"pt\"\n",
    "        ).squeeze()\n",
    "        target_ids = self.tokenizer.encode(\n",
    "            answer, truncation=True, max_length=self.max_length, return_tensors=\"pt\"\n",
    "        ).squeeze()\n",
    "        return {\"input_ids\": input_ids, \"target_ids\": target_ids}\n",
    "\n",
    "# Create train_dataset\n",
    "train_dataset = ConversationDataset(train_pairs, tokenizer)\n",
    "\n",
    "# Step 2: Define collate_fn\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids = [item[\"input_ids\"] for item in batch]\n",
    "    target_ids = [item[\"target_ids\"] for item in batch]\n",
    "\n",
    "    # Calculate max length for padding\n",
    "    max_len = max(max(seq.size(0) for seq in input_ids), max(seq.size(0) for seq in target_ids))\n",
    "\n",
    "    # Pad sequences to the same length\n",
    "    input_ids_padded = pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    target_ids_padded = pad_sequence(target_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "\n",
    "    # Create attention masks (optional for further use)\n",
    "    input_attention_mask = (input_ids_padded != tokenizer.pad_token_id).long()\n",
    "    target_attention_mask = (target_ids_padded != tokenizer.pad_token_id).long()\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids_padded,\n",
    "        \"target_ids\": target_ids_padded,\n",
    "        \"input_attention_mask\": input_attention_mask,\n",
    "        \"target_attention_mask\": target_attention_mask,\n",
    "    }\n",
    "\n",
    "# Step 3: Create train_loader\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "# Step 4: Debugging - Verify batch shapes\n",
    "print(\"\\nDebugging Train Loader:\")\n",
    "for batch in train_loader:\n",
    "    print(\"Batch Input IDs Shape:\", batch[\"input_ids\"].shape)\n",
    "    print(\"Batch Target IDs Shape:\", batch[\"target_ids\"].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9784ace-d6e1-4cb7-9e30-f5a6f205ceb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune_model(model, train_loader, tokenizer, epochs=3, lr=5e-5, gradient_accumulation_steps=2):\n",
    "    \"\"\"\n",
    "    Fine-tune the model using the train_loader and handle padding in target_ids.\n",
    "    \"\"\"\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
    "\n",
    "        for step, batch in enumerate(tqdm(train_loader, desc=\"Training\")):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            target_ids = batch[\"target_ids\"].to(device)\n",
    "\n",
    "            # Mask padding tokens in the loss\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                labels=target_ids.masked_fill(target_ids == tokenizer.pad_token_id, -100)  # Ignore padding tokens\n",
    "            )\n",
    "            loss = outputs.loss / gradient_accumulation_steps\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            if (step + 1) % gradient_accumulation_steps == 0 or (step + 1) == len(train_loader):\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1} Loss: {running_loss / len(train_loader):.4f}\")\n",
    "\n",
    "    # Save the fine-tuned model\n",
    "    model.save_pretrained(\"climate_chatbot\")\n",
    "    tokenizer.save_pretrained(\"climate_chatbot\")\n",
    "    print(\"\\nFine-tuning complete! Model saved as 'climate_chatbot'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "522cfe2f-0d19-4275-af36-c410be8909c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Input IDs Shape: torch.Size([8, 24])\n",
      "Batch Target IDs Shape: torch.Size([8, 128])\n",
      "Target IDs with Padding Masked:\n",
      "tensor([[10734,  4073,  8971,  ...,  1245,   286,  4258],\n",
      "        [  368,  7717,  7313,  ...,   416,   262,  7791],\n",
      "        [ 8117,   389,   734,  ...,  2158,    11,  4258],\n",
      "        ...,\n",
      "        [20123,   278,   257,  ...,  6884,   284,   307],\n",
      "        [ 1169, 45764,   286,  ...,   262,  1109,   326],\n",
      "        [11110,  1112,  3148,  ...,  -100,  -100,  -100]])\n"
     ]
    }
   ],
   "source": [
    "# Verify that padding tokens are being ignored in the target_ids\n",
    "for batch in train_loader:\n",
    "    print(\"Batch Input IDs Shape:\", batch[\"input_ids\"].shape)\n",
    "    print(\"Batch Target IDs Shape:\", batch[\"target_ids\"].shape)\n",
    "    print(\"Target IDs with Padding Masked:\")\n",
    "    print(batch[\"target_ids\"].masked_fill(batch[\"target_ids\"] == tokenizer.pad_token_id, -100))\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28fcad94-8b6a-44c2-bbfb-5959d677cd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the Fine-Tuned Model\n",
    "\n",
    "# Step 7: Validate the Fine-Tuned Model\n",
    "def test_model(model, tokenizer, test_pairs):\n",
    "    \"\"\"\n",
    "    Test the fine-tuned chatbot model on a few test samples.\n",
    "\n",
    "    Args:\n",
    "        model: The fine-tuned model.\n",
    "        tokenizer: The tokenizer used during training.\n",
    "        test_pairs: List of (question, answer) pairs from the test set.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    print(\"\\nTesting the fine-tuned model with sample questions:\")\n",
    "    for i, (question, expected_answer) in enumerate(test_pairs[:5]):  # Test with 5 samples\n",
    "        # Tokenize and encode the question\n",
    "        input_ids = tokenizer.encode(question, return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        # Generate an answer\n",
    "        output_ids = model.generate(\n",
    "            input_ids,\n",
    "            max_length=50,  # Limit the length of the generated answer\n",
    "            num_beams=5,    # Use beam search for better results\n",
    "            early_stopping=True  # Stop when the end token is generated\n",
    "        )\n",
    "        \n",
    "        # Decode the generated answer\n",
    "        predicted_answer = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "        # Print results for evaluation\n",
    "        print(f\"Sample {i + 1}:\")\n",
    "        print(f\"Question: {question}\")\n",
    "        print(f\"Expected Answer: {expected_answer}\")\n",
    "        print(f\"Model's Answer: {predicted_answer}\")\n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "00b5eedc-9411-4b03-b2c8-fe894cc38575",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing the fine-tuned model with sample questions:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1:\n",
      "Question: are humans definitely causing global warming?\n",
      "Expected Answer: just as the world s most respected scientific bodies have confirmed that world is getting hotter, they have also stated that there is strong evidence that humans are driving the warming. the 2005 joint statement from the national academies of brazil, canada, china, france, germany, india, italy, japan, russia, the uk and the us said it is likely that most of the warming in recent decades can be attributed to human activities. countless more recent statements and reports from the world s leading scientific bodies have said the same thing. for example, a 2010 summary of climate science by the royal society stated that there is strong evidence that the warming of the earth over the last half century has been caused largely by human activity, such as the burning of fossil fuels and changes in land use, including agriculture and deforestation. the idea that humans could change the planet s climate may be counter intuitive, but the basic science is well understood. each year, human activity causes billions of tonnes of greenhouse gases to be released into the atmosphere. as scientists have known for decades, these gases capture heat that would otherwise escape to space the equivalent of wrapping the planet in an invisible blanket. of course, the planet s climate has always been in flux thanks to natural factors such as changes in solar or volcanic activity, or cycles relating the earth s orbit around the sun. according to the scientific literature, however, the warming recorded to date matches the pattern of warming we would expect from a build up of greenhouse gas in the atmosphere not the warming we would expect from other possible causes. even if scientists did discover another plausible explanation for the warming observed to date, that would beg a difficult question. as robert henson puts it in the rough guide to climate change if some newly discovered factor can account for the climate change then why aren t carbon dioxide and the other greenhouse gases producing the warming that basic physics tells us they should be? the only way to prove with 100 certainty that humans are responsible for global warming would be to run an experiment with two identical earths one with human influence and one without. that obviously isn t possible, and so most scientists are careful not to state human influence as an absolute certainty. nonetheless, the evidence is now extremely strong.\n",
      "Model's Answer: are humans definitely causing global warming?\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 2:\n",
      "Question: haven t we had global cooling lately?\n",
      "Expected Answer: the planet did cool slightly from the 1940s to the 1970s, mainly in the northern hemisphere and most likely a result of the post war boom in industrial aerosol pollutants that bounce sunlight away from the earth. despite a flurry of 1970s media reports on an imminent ice age, there was never anything approaching a scientific consensus on the likelihood of further cooling, and it appears that greenhouse warming has long since eclipsed the mid century cool spell. after temperatures reached a new global high in 1998, the following decade saw smaller ups and downs. the absence of another show stopping record led many sceptics and pundits to claim that global warming stopped in 1998. in truth, however, nobody expects the global average temperature to rise smoothly from one year to the next. just as any april in london, new york or beijing will see a few cold snaps, we can expect long term warming to be punctuated by periods of little rise in global temperature, or even slight cooling, that could last for a few years. as for record warm years, these will likely coincide with el ni o conditions, which occur as part of a natural cycle in the pacific ocean. in early 2011, the world meteorological organization announced that 2010 was the joint hottest year on record, along with 1998 and 2005.\n",
      "Model's Answer: haven t we had global cooling lately?\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 3:\n",
      "Question: climate is always changing. how do we determine the causes of observed changes?\n",
      "Expected Answer: the causes of observed long term changes in climate on time scales longer than a decade are assessed by determining whether the expected fingerprints of different causes of climate change are present in the historical record. these fingerprints are derived from computer model simulations of the different patterns of climate change caused by individual climate forcings. on multi decade time scales, these forcings include processes such as greenhouse gas increases or changes in solar brightness. by comparing the simulated fingerprint patterns with observed climate changes, we can determine whether observed changes are best explained by those fingerprint patterns, or by natural variability, which occurs without any forcing. the fingerprint of human caused greenhouse gas increases is clearly apparent in the pattern of observed 20th century climate change. the observed change cannot be otherwise explained by the fingerprints of natural forcings or natural variability simulated by climate models. attribution studies therefore support the conclusion that it is extremely likely that human activities have caused more than half of the observed increase in global mean surface temperatures from 1951 to 2010. the earth s climate is always changing, and that can occur for many reasons. to determine the principal causes of observed changes, we must first ascertain whether an observed change in climate is different from other fluctuations that occur without any forcing at all. climate variability without forcing called internal variability is the consequence of processes within the climate system. large scale oceanic variability, such as el ni o southern oscillation enso fluctuations in the pacific ocean, is the dominant source of internal climate variability on decadal to centennial time scales. climate change can also result from natural forcings external to the climate system, such as volcanic eruptions, or changes in the brightness of the sun. forcings such as these are responsible for the huge changes in climate that are clearly documented in the geological record. human caused forcings include greenhouse gas emissions or atmospheric particulate pollution. any of these forcings, natural or human caused, could affect internal variability as well as causing a change in average climate. attribution studies attempt to determine the causes of a detected change in observed climate. over the past century we know that global average temperature has increased, so if the observed change is forced then the principal forcing must be one that causes warming, not cooling. formal climate change attribution studies are carried out using controlled experiments with climate models. the model simulated responses to specific climate forcings are often called the fingerprints of those forcings. a climate model must reliably simulate the fingerprint patterns associated with individual forcings, as well as the patterns of unforced internal variability, in order to yield a meaningful climate change attribution assessment. no model can perfectly reproduce all features of climate, but many detailed studies indicate that simulations using current models are indeed sufficiently reliable to carry out attribution assessments. faq 10.1, figure 1 illustrates part of a fingerprint assessment of global temperature change at the surface during the late 20th century. the observed change in the latter half of the 20th century, shown by the black time series in the left panels, is larger than expected from just internal variability. simulations driven only by natural forcings yellow and blue lines in the upper left panel fail to reproduce late 20th century global warming at the surface with a spatial pattern of change upper right completely different from the observed pattern of change middle right . simulations including both natural and human caused forcings provide a much better representation of the time rate of change lower left and spatial pattern lower right of observed surface temperature change. both panels on the left show that computer models reproduce the naturally forced surface cooling observed for a year or two after major volcanic eruptions, such as occurred in 1982 and 1991. natural forcing simulations capture the short lived temperature changes following eruptions, but only the natural human caused forcing simulations simulate the longer lived warming trend. a more complete attribution assessment would examine temperature above the surface, and possibly other climate variables, in addition to the surface temperature results shown in faq 10.1, figure 1. the fingerprint patterns associated with individual forcings become easier to distinguish when more variables are considered in the assessment. overall, faq 10.1, figure 1 shows that the pattern of observed temperature change is significantly different than the pattern of response to natural forcings alone. the simulated response to all forcings, including human caused forcings, provides a good match to the observed changes at the surface. we cannot correctly simulate recent observed climate change without including the response to human caused forcings, including greenhouse gases, stratospheric ozone, and aerosols. natural causes of change are still at work in the climate system, but recent trends in temperature are largely attributable to human caused forcing.\n",
      "Model's Answer: climate is always changing. how do we determine the causes of observed changes?\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 4:\n",
      "Question: how important is water vapour to climate change?\n",
      "Expected Answer: as the largest contributor to the natural greenhouse effect, water vapour plays an essential role in the earth s climate. however, the amount of water vapour in the atmosphere is controlled mostly by air temperature, rather than by emissions. for that reason, scientists consider it a feedback agent, rather than a forcing to climate change. anthropogenic emissions of water vapour through irrigation or power plant cooling have a negligible impact on the global climate. water vapour is the primary greenhouse gas in the earth s atmosphere. the contribution of water vapour to the natural greenhouse effect relative to that of carbon dioxide co2 depends on the accounting method, but can be considered to be approximately two to three times greater. additional water vapour is injected into the atmosphere from anthropogenic activities, mostly through increased evaporation from irrigated crops, but also through power plant cooling, and marginally through the combustion of fossil fuel. one may therefore question why there is so much focus on co2, and not on water vapour, as a forcing to climate change. water vapour behaves differently from co2 in one fundamental way it can condense and precipitate. when air with high humidity cools, some of the vapour condenses into water droplets or ice particles and precipitates. the typical residence time of water vapour in the atmosphere is ten days. the flux of water vapour into the atmosphere from anthropogenic sources is considerably less than from natural evaporation. therefore, it has a negligible impact on overall concentrations, and does not contribute significantly to the long term greenhouse effect. this is the main reason why tropospheric water vapour typically below 10 km altitude is not considered to be an anthropogenic gas contributing to radiative forcing. anthropogenic emissions do have a significant impact on water vapour in the stratosphere, which is the part of the atmosphere above about 10 km. increased concentrations of methane ch4 due to human activities lead to an additional source of water, through oxidation, which partly explains the observed changes in that atmospheric layer. that stratospheric water change has a radiative impact, is considered a forcing, and can be evaluated. stratospheric concentrations of water have varied significantly in past decades. the full extent of these variations is not well understood and is probably less a forcing than a feedback process added to natural variability. the contribution of stratospheric water vapour to warming, both forcing and feedback, is much smaller than from ch4 or co2. the maximum amount of water vapour in the air is controlled by temperature. a typical column of air extending from the surface to the stratosphere in polar regions may contain only a few kilograms of water vapour per square metre, while a similar column of air in the tropics may contain up to 70 kg. with every extra degree of air temperature, the atmosphere can retain around 7 more water vapour see upper left insert in the faq 8.1, figure 1 . this increase in concentration amplifies the greenhouse effect, and therefore leads to more warming. this process, referred to as the water vapour feedback, is well understood and quantified. it occurs in all models used to estimate climate change, where its strength is consistent with observations. although an increase in atmospheric water vapour has been observed, this change is recognized as a climate feedback from increased atmospheric temperature and should not be interpreted as a radiative forcing from anthropogenic emissions. currently, water vapour has the largest greenhouse effect in the earth s atmosphere. however, other greenhouse gases, primarily co2, are necessary to sustain the presence of water vapour in the atmosphere. indeed, if these other gases were removed from the atmosphere, its temperature would drop sufficiently to induce a decrease of water vapour, leading to a runaway drop of the greenhouse effect that would plunge the earth into a frozen state. so greenhouse gases other than water vapour provide the temperature structure that sustains current levels of atmospheric water vapour. therefore, although co2 is the main anthropogenic control knob on climate, water vapour is a strong and fast feedback that amplifies any initial forcing by a typical factor between two and three. water vapour is not a significant initial forcing, but is nevertheless a fundamental agent of climate change.\n",
      "Model's Answer: how important is water vapour to climate change?\n",
      "--------------------------------------------------\n",
      "Sample 5:\n",
      "Question: how do we know global temperatures are rising?\n",
      "Expected Answer: we know that global temperatures are rising because several independent data sets, made up of direct measurements of the earth s surface temperature, reveal that globally averaged temperatures have warmed by about 1.0 c since 1850 1 . this warming has not happened in a smooth manner, as there are small variations year on year. the climate system is complex and greenhouse gases are not the only factor that contribute to temperature change. in addition to the impact of human activities on the climate, large amounts of energy move around naturally within the climate system, particularly between the atmosphere and ocean. this affects global temperatures over timescales of years by creating temporary warmer or cooler periods, such as during el ni o events. however, the long term temperature pattern since 1850 is clear. the historic trend of atmospheric warming observed for the last 150 years is remarkable and is substantially greater than can be explained by natural variations. the cause of this temperature rise can be explained physically by the increase in atmospheric co2 concentrations that have accompanied it. these have risen from 280 parts per million in around 1850, to over 400 parts per million today. this is a clear indication that human emissions of greenhouse gases are responsible for the rise in global temperatures. natural climate processes could not have caused such a rapid and continued increase in surface temperature 2 . we can be confident in the data. the scientific processes of measuring historical and present day temperature changes have become more refined and scientists regularly carry out careful quality checks to make sure their observations can be trusted and compared over time. this includes making corrections to the time series of temperature records to account for variations in the location or environment of an observation station, or changes in the measurement techniques of historic temperature records. this monitoring makes us certain that increasing global temperatures are due to human activities since the industrial revolution, when the burning of fossil fuels started injecting huge levels of greenhouses gases into the atmosphere.\n",
      "Model's Answer: how do we know global temperatures are rising?\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test the model on the test set\n",
    "test_model(model, tokenizer, test_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ffb2084a-ff0d-4ccc-958e-ceee0e9b8fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot is ready! Type 'exit' or 'quit' to end the session.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  What can we talk about?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: What can we talk about? This is the first I'm hearing of it.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  are humans causing climate change?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: are humans causing climate change?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  are humans causing global warming?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: are humans causing global warming?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  quit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: Goodbye! Have a great day!\n"
     ]
    }
   ],
   "source": [
    "# Build a Real-Time Chatbot Interface\n",
    "\n",
    "# Step 8: Build a Real-Time Chatbot Interface\n",
    "def chatbot_response(question, model, tokenizer, max_length=128, num_beams=5):\n",
    "    \"\"\"\n",
    "    Generate a response to a user's question using the fine-tuned model.\n",
    "\n",
    "    Args:\n",
    "        question (str): User input/question.\n",
    "        model: The fine-tuned model.\n",
    "        tokenizer: The tokenizer used with the model.\n",
    "        max_length (int): Maximum length of the generated response.\n",
    "        num_beams (int): Number of beams for beam search.\n",
    "\n",
    "    Returns:\n",
    "        str: The chatbot's response.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    input_ids = tokenizer.encode(question, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Generate response\n",
    "    output_ids = model.generate(\n",
    "        input_ids, \n",
    "        max_length=max_length, \n",
    "        num_beams=num_beams, \n",
    "        early_stopping=True\n",
    "    )\n",
    "    return tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Chatbot Interaction Loop\n",
    "print(\"Chatbot is ready! Type 'exit' or 'quit' to end the session.\")\n",
    "while True:\n",
    "    try:\n",
    "        user_input = input(\"You: \").strip()  # Trim any leading/trailing spaces\n",
    "        if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "            print(\"Chatbot: Goodbye! Have a great day!\")\n",
    "            break\n",
    "        response = chatbot_response(user_input, model, tokenizer)\n",
    "        print(f\"Chatbot: {response}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "28e21f55-25e2-42d9-862c-0c3837214339",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\milto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Evaluating:   0%|          | 0/38 [00:00<?, ?it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating:   3%|▎         | 1/38 [00:01<00:37,  1.02s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating:   5%|▌         | 2/38 [00:01<00:27,  1.33it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating:   8%|▊         | 3/38 [00:02<00:25,  1.40it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating:  11%|█         | 4/38 [00:02<00:23,  1.44it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating:  13%|█▎        | 5/38 [00:03<00:21,  1.51it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating:  16%|█▌        | 6/38 [00:04<00:22,  1.45it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating:  18%|█▊        | 7/38 [00:04<00:19,  1.57it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating:  21%|██        | 8/38 [00:05<00:16,  1.80it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating:  24%|██▎       | 9/38 [00:05<00:15,  1.83it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating:  26%|██▋       | 10/38 [00:06<00:17,  1.63it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating:  29%|██▉       | 11/38 [00:06<00:15,  1.73it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating:  32%|███▏      | 12/38 [00:07<00:13,  1.91it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating:  34%|███▍      | 13/38 [00:07<00:13,  1.91it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating:  37%|███▋      | 14/38 [00:09<00:16,  1.41it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating:  39%|███▉      | 15/38 [00:09<00:15,  1.50it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating:  42%|████▏     | 16/38 [00:10<00:13,  1.62it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating:  45%|████▍     | 17/38 [00:10<00:13,  1.55it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating:  47%|████▋     | 18/38 [00:11<00:10,  1.83it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating:  50%|█████     | 19/38 [00:11<00:09,  1.98it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating:  53%|█████▎    | 20/38 [00:11<00:08,  2.09it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating:  55%|█████▌    | 21/38 [00:12<00:09,  1.79it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating:  58%|█████▊    | 22/38 [00:13<00:08,  1.80it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating:  61%|██████    | 23/38 [00:13<00:08,  1.83it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating:  63%|██████▎   | 24/38 [00:14<00:07,  1.77it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating:  66%|██████▌   | 25/38 [00:14<00:06,  1.87it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating:  68%|██████▊   | 26/38 [00:15<00:07,  1.57it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating:  71%|███████   | 27/38 [00:16<00:07,  1.51it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating:  74%|███████▎  | 28/38 [00:17<00:06,  1.43it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating:  76%|███████▋  | 29/38 [00:17<00:06,  1.48it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating:  79%|███████▉  | 30/38 [00:18<00:05,  1.45it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating:  82%|████████▏ | 31/38 [00:19<00:05,  1.29it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating:  84%|████████▍ | 32/38 [00:20<00:04,  1.32it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating:  87%|████████▋ | 33/38 [00:20<00:03,  1.47it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating:  89%|████████▉ | 34/38 [00:21<00:02,  1.39it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating:  92%|█████████▏| 35/38 [00:22<00:02,  1.48it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating:  95%|█████████▍| 36/38 [00:22<00:01,  1.55it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating:  97%|█████████▋| 37/38 [00:23<00:00,  1.58it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating: 100%|██████████| 38/38 [00:24<00:00,  1.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average BLEU Score: 0.0050\n",
      "Average BLEU Score: 0.0050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Model Performance\n",
    "\n",
    "# Ensure required NLTK resources are downloaded\n",
    "nltk.download('punkt')\n",
    "\n",
    "# BLEU evaluation function\n",
    "def compute_bleu_score(pairs, model, tokenizer, device, max_length=128):\n",
    "    model.eval()\n",
    "    bleu_scores = []\n",
    "    smooth_fn = SmoothingFunction().method4\n",
    "\n",
    "    for question, reference in tqdm(pairs, desc=\"Evaluating\"):\n",
    "        input_ids = tokenizer.encode(question, return_tensors=\"pt\").to(device)\n",
    "        output_ids = model.generate(input_ids, max_length=max_length, num_beams=5, early_stopping=True)\n",
    "        predicted_answer = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "        reference_tokens = nltk.word_tokenize(reference)\n",
    "        predicted_tokens = nltk.word_tokenize(predicted_answer)\n",
    "        bleu = sentence_bleu([reference_tokens], predicted_tokens, smoothing_function=smooth_fn)\n",
    "        bleu_scores.append(bleu)\n",
    "\n",
    "    avg_bleu = sum(bleu_scores) / len(bleu_scores)\n",
    "    print(f\"Average BLEU Score: {avg_bleu:.4f}\")\n",
    "    return avg_bleu\n",
    "\n",
    "# Define device for computation\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Run BLEU evaluation\n",
    "average_bleu = compute_bleu_score(test_pairs, model, tokenizer, device)\n",
    "print(f\"Average BLEU Score: {average_bleu:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2f001afc-7c32-43a5-9a89-5ccf862aa5b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned model saved successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot pipeline saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Save Model and Artifacts for Deployment\n",
    "\n",
    "# Save model and tokenizer\n",
    "model.save_pretrained(\"climate_chatbot\")\n",
    "tokenizer.save_pretrained(\"climate_chatbot\")\n",
    "print(\"Fine-tuned model saved successfully!\")\n",
    "\n",
    "# Export to Hugging Face format for deployment\n",
    "from transformers import pipeline\n",
    "chatbot_pipeline = pipeline(\"text-generation\", model=\"climate_chatbot\", tokenizer=\"climate_chatbot\")\n",
    "chatbot_pipeline.save_pretrained(\"climate_chatbot_pipeline\")\n",
    "print(\"Chatbot pipeline saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3896626-6039-46fc-a1d1-990ac92a50a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next Steps:\n",
    "\n",
    "# Deploy:\n",
    "# Use the saved pipeline in a web application, API, or cloud-based service for real-world interaction.\n",
    "# Hugging Face's transformers library makes deploying in a Flask or FastAPI server straightforward.\n",
    "\n",
    "# Iterate on Fine-Tuning:\n",
    "# Improve performance by testing hyperparameter changes or adding more relevant training data.\n",
    "\n",
    "# Optimize Responses:\n",
    "# Integrate response filtering or re-ranking to improve user interactions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (torch_env)",
   "language": "python",
   "name": "torch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
